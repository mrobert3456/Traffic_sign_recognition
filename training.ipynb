{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Course:  Convolutional Neural Networks for Image Classification\n",
    "\n",
    "## Section-6\n",
    "### Train designed CNNs models in Keras\n",
    "\n",
    "**Description:**  \n",
    "*Run training process for all developed models and all prepared datasets  \n",
    "Save trained models and trained weights*\n",
    "\n",
    "**File:** *training.ipynb*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm:\n",
    "\n",
    "**--> Step 1:** Load saved CNN model  \n",
    "**--> Step 2:** Set up learning rate & epochs  \n",
    "**--> Step 3: Train loaded model on all preprocessed datasets**  \n",
    "**--> Step 4:** Show and plot results  \n",
    "\n",
    "\n",
    "**Result:**  \n",
    "- Binary files with saved weights  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing needed libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "\n",
    "from keras.models import load_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint, Callback\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up full paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full or absolute path to 'Section4' with preprocessed datasets\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section4'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section4'\n",
    "full_path_to_Section4 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section4'\n",
    "\n",
    "\n",
    "# Full or absolute path to 'Section5' with designed models\n",
    "# (!) On Windows, the path should look like following:\n",
    "# r'C:\\Users\\your_name\\PycharmProjects\\CNNCourse\\Section5'\n",
    "# or:\n",
    "# 'C:\\\\Users\\\\your_name\\\\PycharmProjects\\\\CNNCourse\\\\Section5'\n",
    "full_path_to_Section5 = \\\n",
    "    '/home/valentyn/PycharmProjects/CNNCourse/Section5'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_1_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_1_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 5)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'custom' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'custom' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all custom datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all custom datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.61, 0.694)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'validation_model_1_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all custom datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'losses_model_1_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for custom dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'custom' + '/' + \n",
    "                                'model_2_custom_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'custom' + '/' + \n",
    "                                 'model_2_custom_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_custom_rgb_255_mean.hdf5',\n",
    "            'dataset_custom_rgb_255_mean_std.hdf5',\n",
    "            'dataset_custom_gray_255_mean.hdf5',\n",
    "            'dataset_custom_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all custom datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved custom dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'custom' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 5)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 5)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'custom' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'custom' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 10th batch\n",
    "            if batch % 10 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'custom' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all custom datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all custom datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.4, 0.615)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'validation_model_2_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all custom datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for Custom Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('custom' + '/' + 'losses_model_2_custom_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_1_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_1_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):    \n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'cifar10' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'cifar10' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all CIFAR-10 datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all CIFAR-10 datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.71, 0.84)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'validation_model_1_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all CIFAR-10 datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'losses_model_1_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for CIFAR-10 dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'cifar10' + '/' + \n",
    "                                'model_2_cifar10_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'cifar10' + '/' + \n",
    "                                 'model_2_cifar10_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_cifar10_rgb_255_mean.hdf5',\n",
    "            'dataset_cifar10_rgb_255_mean_std.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean.hdf5',\n",
    "            'dataset_cifar10_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all CIFAR-10 datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved CIFAR-10 dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'cifar10' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'cifar10' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'cifar10' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'cifar10' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CIFAR-10 dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all CIFAR-10 datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all CIFAR-10 datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.5, 0.72)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'validation_model_2_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all CIFAR-10 datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='medium')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for CIFAR-10 Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('cifar10' + '/' + 'losses_model_2_cifar10_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining list to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + 'model_1_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all MNIST datasets in a loop\n",
    "for i in range(2):    \n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'mnist' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'mnist' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Training GRAY model with current dataset\n",
    "    temp = model_gray[i].fit(x_train, y_train,\n",
    "                             batch_size=50,\n",
    "                             epochs=epochs,\n",
    "                             validation_data=(x_validation, y_validation),\n",
    "                             callbacks=[learning_rate, best_weights],\n",
    "                             verbose=1)\n",
    "\n",
    "    \n",
    "    # Adding results of 1st model for current GRAY dataset in the list\n",
    "    h.append(temp)\n",
    "    \n",
    "    \n",
    "    # Check points\n",
    "    print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "    print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all MNIST datasets for 1st model\n",
    "for i in range(2):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all MNIST datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.989, 0.995)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'validation_model_1_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all MNIST datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='upper right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'losses_model_1_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for MNIST dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'mnist' + '/' + \n",
    "                                 'model_2_mnist_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing model's input shape\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_mnist_gray_255_mean.hdf5',\n",
    "            'dataset_mnist_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all MNIST datasets in a loop\n",
    "for i in range(2):\n",
    "    # Opening saved MNIST dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'mnist' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 10)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 10)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'mnist' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'mnist' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'mnist' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if second GRAY dataset is opened\n",
    "    if i == 1:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i].fit(x_train, y_train,\n",
    "                                 batch_size=50,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_validation, y_validation),\n",
    "                                 callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                 verbose=1)\n",
    "        \n",
    "    # Checking if first GRAY dataset is opened\n",
    "    else:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i].fit(x_train, y_train,\n",
    "                                 batch_size=50,\n",
    "                                 epochs=epochs,\n",
    "                                 validation_data=(x_validation, y_validation),\n",
    "                                 callbacks=[learning_rate, best_weights],\n",
    "                                 verbose=1)\n",
    "\n",
    "        \n",
    "    # Adding results of 2nd model for current GRAY dataset in the list\n",
    "    h.append(temp)\n",
    "    \n",
    "    \n",
    "    # Check points\n",
    "    print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "    print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MNIST dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all MNIST datasets for 2nd model\n",
    "for i in range(2):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all MNIST datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.985, 0.993)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'validation_model_2_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all MNIST datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for MNIST Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('mnist' + '/' + 'losses_model_2_mnist_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 1: Loading saved 1st model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 1st model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'ts' + '/' + \n",
    "                                'model_1_ts_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'ts' + '/' + \n",
    "                                 'model_1_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 1st model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 43)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    " \n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'ts' + '/' + 'w_1' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'ts' + '/' + 'w_1' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Training RGB model with current dataset\n",
    "        temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                batch_size=50,\n",
    "                                epochs=epochs,\n",
    "                                validation_data=(x_validation, y_validation),\n",
    "                                callbacks=[learning_rate, best_weights],\n",
    "                                verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 1st model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('1st model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_1' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 1st model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all Traffic Signs datasets for 1st model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all Traffic Signs datasets for 1st model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.97, 0.9992)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 1st model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'validation_model_1_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all Traffic Signs datasets for 1st model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 1st model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'losses_model_1_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 1: Loading saved 2nd model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining lists to collect models in\n",
    "model_rgb = []\n",
    "model_gray = []\n",
    "\n",
    "\n",
    "# Loading 2nd model for Traffic Signs dataset\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "for i in range(2):\n",
    "    model_rgb.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                'ts' + '/' + \n",
    "                                'model_2_ts_rgb.h5'))\n",
    "    \n",
    "    model_gray.append(load_model(full_path_to_Section5 + '/' + \n",
    "                                 'ts' + '/' + \n",
    "                                 'model_2_ts_gray.h5'))\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Models are successfully loaded')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing models' input shapes\n",
    "print(model_rgb[0].layers[0].input_shape)\n",
    "print()\n",
    "print(model_gray[0].layers[0].input_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 2: Setting up learning rate & epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining number of epochs\n",
    "epochs = 50\n",
    "\n",
    "\n",
    "# Defining schedule to update learning rate\n",
    "learning_rate = LearningRateScheduler(lambda x: 1e-2 * 0.95 ** (x + epochs), verbose=1)\n",
    "\n",
    "\n",
    "# Check point\n",
    "print('Number of epochs and schedule for learning rate are set successfully')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 3: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# If you're using Nvidia GPU and 'cnngpu' environment, there might be an issue like:\n",
    "'''Failed to get convolution algorithm. This is probably because cuDNN failed to initialize'''\n",
    "# In this case, close all Jupyter Notebooks, close Terminal Window or Anaconda Prompt\n",
    "# Open again just this one Jupyter Notebook and run it\n",
    "\n",
    "\n",
    "# Preparing list with datasets' names\n",
    "datasets = ['dataset_ts_rgb_255_mean.hdf5',\n",
    "            'dataset_ts_rgb_255_mean_std.hdf5',\n",
    "            'dataset_ts_gray_255_mean.hdf5',\n",
    "            'dataset_ts_gray_255_mean_std.hdf5']\n",
    "\n",
    "\n",
    "# Defining list to collect results in\n",
    "h = []\n",
    "\n",
    "\n",
    "# Training 2nd model with all Traffic Signs datasets in a loop\n",
    "for i in range(4):\n",
    "    # Opening saved Traffic Signs dataset from HDF5 binary file\n",
    "    # Initiating File object\n",
    "    # Opening file in reading mode by 'r'\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    with h5py.File(full_path_to_Section4 + '/' + 'ts' + '/' + datasets[i], 'r') as f:\n",
    "        # Extracting saved arrays for training by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_train = f['x_train']  # HDF5 dataset\n",
    "        y_train = f['y_train']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_train = np.array(x_train)  # Numpy arrays\n",
    "        y_train = np.array(y_train)  # Numpy arrays\n",
    "\n",
    "        # Extracting saved arrays for validation by appropriate keys\n",
    "        # Saving them into new variables\n",
    "        x_validation = f['x_validation']  # HDF5 dataset\n",
    "        y_validation = f['y_validation']  # HDF5 dataset\n",
    "        # Converting them into Numpy arrays\n",
    "        x_validation = np.array(x_validation)  # Numpy arrays\n",
    "        y_validation = np.array(y_validation)  # Numpy arrays\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Following dataset is successfully opened:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Preparing classes to be passed into the model\n",
    "    # Transforming them from vectors to binary matrices\n",
    "    # It is needed to set relationship between classes to be understood by the algorithm\n",
    "    # Such format is commonly used in training and predicting\n",
    "    y_train = to_categorical(y_train, num_classes = 43)\n",
    "    y_validation = to_categorical(y_validation, num_classes = 43)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Binary matrices are successfully created:        ', datasets[i])\n",
    "\n",
    "\n",
    "    # Preparing filepath to save best weights\n",
    "    # (!) On Windows, it might need to change\n",
    "    # this: + '/' +\n",
    "    # to this: + '\\' +\n",
    "    # or to this: + '\\\\' +\n",
    "    best_weights_filepath = 'ts' + '/' + 'w_2' + datasets[i][7:-5] + '.h5'\n",
    "    \n",
    "    # Formatting options to save all weights for every epoch\n",
    "    # 'ts' + '/' + 'w_2' + datasets[i][7:-5] + '_{epoch:02d}_{val_accuracy:.4f}' + '.h5'\n",
    "    \n",
    "    # Defining schedule to save best weights\n",
    "    best_weights = ModelCheckpoint(filepath=best_weights_filepath,\n",
    "                                   save_weights_only=True,                                   \n",
    "                                   monitor='val_accuracy',\n",
    "                                   mode='max',\n",
    "                                   save_best_only=True,\n",
    "                                   period=1,\n",
    "                                   verbose=1)\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save best weights is created:        ', datasets[i])\n",
    "    \n",
    "    \n",
    "    # Defining schedule to save intermediate weights\n",
    "    class CustomCallback(Callback):\n",
    "        # Constructor of the class\n",
    "        def __init__(self):\n",
    "            # Defining variable to be as a part of filename\n",
    "            self.filename = 0\n",
    "        \n",
    "        # Function that is called at the end of every batch\n",
    "        def on_train_batch_end(self, batch, logs=None):\n",
    "            # Checking if it is every 100th batch\n",
    "            if batch % 100 == 0:\n",
    "                # Preparing filepath to save intermediate weights\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                intermediate_weights_filepath = 'ts' + '/' + \\\n",
    "                                                'intermediate' + '/' + \\\n",
    "                                                '{0:04d}'.format(self.filename) + \\\n",
    "                                                '_w_2' + \\\n",
    "                                                datasets[i][7:-5] + '.h5'\n",
    "                \n",
    "                # Getting weights only for the first convolutional layer\n",
    "                weights_layer_0 = self.model.get_weights()[0]\n",
    "                \n",
    "                # Saving obtained weights into new HDF5 binary file\n",
    "                # Initiating File object\n",
    "                # Creating file with current name\n",
    "                # Opening it in writing mode by 'w'\n",
    "                # (!) On Windows, it might need to change\n",
    "                # this: + '/' +\n",
    "                # to this: + '\\' +\n",
    "                # or to this: + '\\\\' +\n",
    "                with h5py.File(intermediate_weights_filepath, 'w') as f:\n",
    "                    # Calling method to create dataset of given shape and type\n",
    "                    # Saving Numpy array with weights from the first layer\n",
    "                    f.create_dataset('weights_layer_0', data=weights_layer_0, dtype='f')\n",
    "                    \n",
    "                # Increasing variable to be as a part of the next filename\n",
    "                self.filename += 1\n",
    "    \n",
    "    \n",
    "    # Check point\n",
    "    print('Schedule to save intermediate weights is created:', datasets[i])\n",
    "\n",
    "    \n",
    "    # Checking if RGB dataset is opened\n",
    "    if i <= 1:\n",
    "        # Checking if second RGB dataset is opened\n",
    "        if i == 1:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights, CustomCallback()],\n",
    "                                    verbose=1)\n",
    "        \n",
    "        # Checking if first RGB dataset is opened\n",
    "        else:\n",
    "            # Training RGB model with current dataset\n",
    "            temp = model_rgb[i].fit(x_train, y_train,\n",
    "                                    batch_size=50,\n",
    "                                    epochs=epochs,\n",
    "                                    validation_data=(x_validation, y_validation),\n",
    "                                    callbacks=[learning_rate, best_weights],\n",
    "                                    verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current RGB dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for RGB is successfully trained on:    ', datasets[i])\n",
    "        print('Trained weights for RGB are saved successfully:  ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n",
    "    \n",
    "    # Checking if GRAY dataset is opened\n",
    "    elif i >= 2:\n",
    "        # Training GRAY model with current dataset\n",
    "        temp = model_gray[i-2].fit(x_train, y_train,\n",
    "                                   batch_size=50,\n",
    "                                   epochs=epochs,\n",
    "                                   validation_data=(x_validation, y_validation),\n",
    "                                   callbacks=[learning_rate, best_weights],\n",
    "                                   verbose=1)\n",
    "\n",
    "        \n",
    "        # Adding results of 2nd model for current GRAY dataset in the list\n",
    "        h.append(temp)\n",
    "        \n",
    "        \n",
    "        # Check points\n",
    "        print('2nd model for GRAY is successfully trained on:   ', datasets[i])\n",
    "        print('Trained weights for GRAY are saved successfully: ', 'w_2' + datasets[i][7:-5] + '.h5')\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Traffic Signs dataset, 2nd model\n",
    "\n",
    "## Step 4: Showing and plotting results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resulted accuracies of all Traffic Signs datasets for 2nd model\n",
    "for i in range(4):\n",
    "    print('T: {0:.5f},  V: {1:.5f},  D: {2}'.format(max(h[i].history['accuracy']),\n",
    "                                                    max(h[i].history['val_accuracy']),\n",
    "                                                    datasets[i][8:-5]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing other parameters that history holds\n",
    "print(h[0].params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting accuracies of all Traffic Signs datasets for 2nd model\n",
    "plt.plot(h[0].history['val_accuracy'], '-o')\n",
    "plt.plot(h[1].history['val_accuracy'], '-o')\n",
    "plt.plot(h[2].history['val_accuracy'], '-o')\n",
    "plt.plot(h[3].history['val_accuracy'], '-o')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='lower right',\n",
    "           fontsize='xx-large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "\n",
    "\n",
    "# Setting limit along Y axis\n",
    "plt.ylim(0.88, 0.994)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Validation accuracies of 2nd model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'validation_model_2_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Setting default size of the plot\n",
    "plt.rcParams['figure.figsize'] = (12.0, 6.0)\n",
    "\n",
    "\n",
    "# Plotting training and validation losses of all Traffic Signs datasets for 2nd model\n",
    "plt.plot(h[0].history['loss'], '-ob')\n",
    "plt.plot(h[1].history['loss'], '-og')\n",
    "plt.plot(h[2].history['loss'], '-or')\n",
    "plt.plot(h[3].history['loss'], '-oc')\n",
    "\n",
    "plt.plot(h[0].history['val_loss'], '-ob')\n",
    "plt.plot(h[1].history['val_loss'], '-og')\n",
    "plt.plot(h[2].history['val_loss'], '-or')\n",
    "plt.plot(h[3].history['val_loss'], '-oc')\n",
    "\n",
    "\n",
    "# Showing legend\n",
    "plt.legend(['rgb_255_mean', 'rgb_255_mean_std',\n",
    "            'gray_255_mean', 'gray_255_mean_std'],\n",
    "           loc='center right',\n",
    "           fontsize='large')\n",
    "\n",
    "\n",
    "# Giving name to axes\n",
    "plt.xlabel('Epoch', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "\n",
    "\n",
    "# Giving name to the plot\n",
    "plt.title('Losses of 2nd model for Traffic Signs Datasets', fontsize=16)\n",
    "\n",
    "\n",
    "# Saving plot\n",
    "# (!) On Windows, it might need to change\n",
    "# this: + '/' +\n",
    "# to this: + '\\' +\n",
    "# or to this: + '\\\\' +\n",
    "plt.savefig('ts' + '/' + 'losses_model_2_ts_dataset.png', dpi=500)\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Showing list of scheduled learning rate for every epoch\n",
    "print(h[0].history['lr'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Magic function that renders the figure in a jupyter notebook\n",
    "# instead of displaying a figure object\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "# Plotting scheduled learning rate\n",
    "plt.plot(h[0].history['lr'], '-mo')\n",
    "\n",
    "\n",
    "# Showing the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments\n",
    "\n",
    "To get more details for usage of 'ModelCheckpoint' class:  \n",
    "**print(help(ModelCheckpoint))**  \n",
    "  \n",
    "More details and examples are here:  \n",
    " - https://keras.io/api/callbacks/  \n",
    " - https://keras.io/api/callbacks/model_checkpoint/  \n",
    " - https://keras.io/guides/writing_your_own_callbacks/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(help(ModelCheckpoint))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
